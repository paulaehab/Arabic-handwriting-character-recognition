# -*- coding: utf-8 -*-
"""train_test_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J_HsZPfzJZTf30RYS4OtwfduI6D_-Pam
"""

# Commented out IPython magic to ensure Python compatibility.
# Import main libraries necessary for this project
import numpy as np
import pandas as pd
from IPython.display import display # Allows the use of display() for DataFrames

# Import libraries needed for reading image and processing it
import csv
from PIL import Image
from scipy.ndimage import rotate

# Pretty display for notebooks
# %matplotlib inline

"""# Load Arabic Characters Dataset"""

# Training letters images url
url1 = "https://drive.google.com/file/d/1-J3cfxCkXNWtSai-06gXxGEgNrZmJhIz/view?usp=sharing"
letters_training_images_file_path= 'https://drive.google.com/uc?export=download&id='+url1.split('/')[-2]
# Training letters labels url
url2 = "https://drive.google.com/file/d/1oTJRQlRkWYliTgCEc4EHFiPg4_TDn5Zs/view?usp=sharing"
letters_training_labels_file_path = 'https://drive.google.com/uc?export=download&id='+url2.split('/')[-2]
# Testing letters images url
url3 ="https://drive.google.com/file/d/1EPbu952e7oCxcL6l38_HxqF_y_ajh42G/view?usp=sharing"
letters_testing_images_file_path = 'https://drive.google.com/uc?export=download&id='+url3.split('/')[-2]
# Testing letters labels url
url4 ="https://drive.google.com/file/d/1SkoNNi_1HVhuS7CSi9OC8UtJaIUEpyzZ/view?usp=sharing"
letters_testing_labels_file_path = 'https://drive.google.com/uc?export=download&id='+url4.split('/')[-2]


# Loading dataset into dataframes
training_letters = pd.read_csv(letters_training_images_file_path, compression='zip', header=None)
training_letters_labels = pd.read_csv(letters_training_labels_file_path, compression='zip', header=None)
testing_letters = pd.read_csv(letters_testing_images_file_path, compression='zip', header=None)
testing_letters_labels = pd.read_csv(letters_testing_labels_file_path, compression='zip', header=None)

# print statistics about the dataset
print("There are %d training arabic letter images of 64x64 pixels." %training_letters.shape[0])
print("There are %d testing arabic letter images of 64x64 pixels." %testing_letters.shape[0])
training_letters.head()

"""# convert images from values to images """

def convert_pixels_to_image(image_values, display=False):
    
# put image values into numpy array
    image_array = np.asarray(image_values)
#reshape the image to be 64*64 image
    image_array = image_array.reshape(64, 64).astype('uint8')
# flip the images as tha dataset is reflected
    image_array = np.flip(image_array, 0)
# rotate the image -90 degree to appear right
    image_array = rotate(image_array, -90)
# from the image from thge array of pixels 
    new_image = Image.fromarray(image_array)
    if display == True:
        new_image.show()
    return new_image

"""# Start to visulalize so images from dataset"""

convert_pixels_to_image(training_letters.loc[10], True)

convert_pixels_to_image(training_letters.loc[200], True)

convert_pixels_to_image(training_letters.loc[40], True)

"""## Data Preprocessing

### Image Normalization
normalizing mean : to change the intenisty range of pixesl here we will change it from range (0-255)
to (0,1) range only to do this we will divide each pixel by 255
"""

training_letters_scaled = training_letters.values.astype('float32')/255
training_letters_labels = training_letters_labels.values.astype('int32')
testing_letters_scaled = testing_letters.values.astype('float32')/255
testing_letters_labels = testing_letters_labels.values.astype('int32')

print("Training images of letters after scaling")
print(training_letters_scaled.shape)
training_letters_scaled[0:5]

"""# Encoding the labels to categories 
From the labels csv files we can see that labels are categorical values and it is a multi-class classification problem.

Our outputs are in the form of:

Letters from ’alef’ to ’yeh’ have categories numbers from 10 to 37
Here we will encode these categories values using One Hot Encoding with keras.

## One-hot encoding: : transforms integer to a binary matrix where the array contains only one ‘1’ and the rest elements are ‘0’.
"""

from tensorflow.keras.utils import to_categorical

# one hot encoding
# number of classes = 10 for digits + 28 for charcters 
number_of_classes = 38
training_letters_labels_encoded = to_categorical(training_letters_labels, num_classes=number_of_classes)
testing_letters_labels_encoded = to_categorical(testing_letters_labels, num_classes=number_of_classes)

print(training_letters_labels_encoded)

"""### Reshaping Input Images to 64x64x1

When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape (nb_samples,rows,columns,channels)

where nb_samples corresponds to the total number of images (or samples), and rows, columns, and channels correspond to the number of rows, columns, and channels for each image, respectively.  
**So we will reshape the input images to a 4D tensor with shape
(nb_samples, 64, 64 ,1)** as we use grayscale images of 64x64 pixels.
"""

training_letters_scaled = training_letters_scaled.reshape([-1, 64, 64, 1])
testing_letters_scaled = testing_letters_scaled.reshape([-1, 64, 64, 1])

print(training_letters_scaled.shape, training_letters_labels_encoded.shape, testing_letters_scaled.shape, testing_letters_labels_encoded.shape)

"""## Designing Model Architecture

Now we will make a method which creates the model architecture with the specified optimizer and activation functions.
"""

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Dropout, Dense

def create_model(optimizer='adam', kernel_initializer='he_normal', activation='relu'):
  # create model
  model = Sequential()
  model.add(Conv2D(filters=16, kernel_size=3, padding='same', input_shape=(64, 64, 1), kernel_initializer=kernel_initializer, activation=activation))
  model.add(BatchNormalization())
  model.add(MaxPooling2D(pool_size=2))
  model.add(Dropout(0.2))

  model.add(Conv2D(filters=32, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))
  model.add(BatchNormalization())
  model.add(MaxPooling2D(pool_size=2))
  model.add(Dropout(0.2))

  model.add(Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))
  model.add(BatchNormalization())
  model.add(MaxPooling2D(pool_size=2))
  model.add(Dropout(0.2))

  model.add(Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))
  model.add(BatchNormalization())
  model.add(MaxPooling2D(pool_size=2))
  model.add(Dropout(0.2))
  model.add(GlobalAveragePooling2D())
  
  #Fully connected final layer
  model.add(Dense(38, activation='softmax'))

  # Compile model
  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=optimizer)
  return model

model = create_model(optimizer='Adam', kernel_initializer='uniform', activation='relu')

from keras.callbacks import ModelCheckpoint  

# using checkpoints to save model weights to be used later instead of training again on the same epochs.
checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)

history = model.fit(training_letters_scaled, training_letters_labels_encoded, 
                    validation_data=(testing_letters_scaled, testing_letters_labels_encoded),
                    epochs=20, batch_size=20, verbose=1, callbacks=[checkpointer])

# Final evaluation of the model
metrics = model.evaluate(testing_letters_scaled, testing_letters_labels_encoded, verbose=1)
print("Test Accuracy: {}".format(metrics[1]))
print("Test Loss: {}".format(metrics[0]))

"""# saving the model"""

model.save('my_model.h5')

"""# load model for testing"""

from tensorflow import keras
import tensorflow as tf

# Recreate the exact same model, including its weights and the optimizer
new_model = tf.keras.models.load_model('my_model.h5')

# Show the model architecture
new_model.summary()

"""# Build function to get the prediction from the model



"""

def get_predicted_classes(model, data, labels=None):
  # parameters are the data of image we want to predict 
  image_predictions = new_model.predict(data)
  #Get the most valued predicted class from the model
  predicted_classes = np.argmax(image_predictions, axis=1)
  #Get the label of the predicted class
  true_classes = np.argmax(labels, axis=1)
  return predicted_classes, true_classes

"""# Build function to reverse the one hot encoding that we do with the labels
* Letters from ’alef’='أ' to ’yeh’='ى' were encoded to categorical labels from 10 to 37

**Recall that we have used One Hot Encoding which transforms integer to a binary matrix where the array contains only one ‘1’ and the rest elements are ‘0’.**
"""

def get_non_zero_index_from_one_hot_encoding(one_hot_encoding):
  non_zero_index = np.where(one_hot_encoding == 1)[0]
  assert(len(non_zero_index) == 1)
  non_zero_index = non_zero_index[0]
  return non_zero_index

"""# Build function to convert categorial label to label from data """

def convert_categorical_label_to_real_label(categorical_label):
  #intial empty list
  real_labels = []
  real_labels.extend([x for x in range(10)])

  # Add the Arbaic letters into a list
  real_labels.extend(['أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى'])
  return real_labels[categorical_label]

def show_example(sample_index):
  #get predict class and the true predict label from output of the model 
  #Using get_predicted_classes()function
  y_pred, y_true = get_predicted_classes(model, testing_letters_scaled[[sample_index]], testing_letters_labels_encoded[[sample_index]])
 #Get the real label of data from data set of the image we want the model to predict it 
  non_zero_index = get_non_zero_index_from_one_hot_encoding(testing_letters_labels_encoded[sample_index])
  #the true label from data 
  y_true = y_true[0]
  #the predicted label from model
  y_pred = y_pred[0]
  # check if the true label == the label from dataset after the inverse of hot encoder
  assert y_true == non_zero_index
  #return the label of the true label in Arabic letters
  true_label = convert_categorical_label_to_real_label(y_true)
  #return the label of the predicted label in Arabic letters

  predicted_label = convert_categorical_label_to_real_label(y_pred)
  # print the predictad and the true label 
  print("The following image has the written character '{}' but the model predicted it as '{}'".format(true_label, predicted_label))
  return true_label == predicted_label

"""# Start to test the model 
Range of indcies from (0:3360)
"""

sample_index = 200
show_example(sample_index)
convert_pixels_to_image(testing_letters.loc[sample_index], True)

sample_index = 3
show_example(sample_index)
convert_pixels_to_image(testing_letters.loc[sample_index ], True)