{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Import libraries needed for reading image and processing it\n",
    "import csv\n",
    "from PIL import Image\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Arabic Characters Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13440 training arabic letter images of 64x64 pixels.\n",
      "There are 3360 testing arabic letter images of 64x64 pixels.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  4086  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   4087  4088  4089  4090  4091  4092  4093  4094  4095  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 4096 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Training letters images url\n",
    "url1 = \"https://drive.google.com/file/d/1-J3cfxCkXNWtSai-06gXxGEgNrZmJhIz/view?usp=sharing\"\n",
    "letters_training_images_file_path= 'https://drive.google.com/uc?export=download&id='+url1.split('/')[-2]\n",
    "# Training letters labels url\n",
    "url2 = \"https://drive.google.com/file/d/1oTJRQlRkWYliTgCEc4EHFiPg4_TDn5Zs/view?usp=sharing\"\n",
    "letters_training_labels_file_path = 'https://drive.google.com/uc?export=download&id='+url2.split('/')[-2]\n",
    "# Testing letters images url\n",
    "url3 =\"https://drive.google.com/file/d/1EPbu952e7oCxcL6l38_HxqF_y_ajh42G/view?usp=sharing\"\n",
    "letters_testing_images_file_path = 'https://drive.google.com/uc?export=download&id='+url3.split('/')[-2]\n",
    "# Testing letters labels url\n",
    "url4 =\"https://drive.google.com/file/d/1SkoNNi_1HVhuS7CSi9OC8UtJaIUEpyzZ/view?usp=sharing\"\n",
    "letters_testing_labels_file_path = 'https://drive.google.com/uc?export=download&id='+url4.split('/')[-2]\n",
    "\n",
    "\n",
    "# Loading dataset into dataframes\n",
    "training_letters = pd.read_csv(letters_training_images_file_path, compression='zip', header=None)\n",
    "training_letters_labels = pd.read_csv(letters_training_labels_file_path, compression='zip', header=None)\n",
    "testing_letters = pd.read_csv(letters_testing_images_file_path, compression='zip', header=None)\n",
    "testing_letters_labels = pd.read_csv(letters_testing_labels_file_path, compression='zip', header=None)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print(\"There are %d training arabic letter images of 64x64 pixels.\" %training_letters.shape[0])\n",
    "print(\"There are %d testing arabic letter images of 64x64 pixels.\" %testing_letters_.shape[0])\n",
    "training_letters.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert images from values to images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pixels_to_image(image_values, display=False):\n",
    "    \n",
    "# put image values into numpy array\n",
    "    image_array = np.asarray(image_values)\n",
    "#reshape the image to be 64*64 image\n",
    "    image_array = image_array.reshape(64, 64).astype('uint8')\n",
    "# flip the images as tha dataset is reflected\n",
    "    image_array = np.flip(image_array, 0)\n",
    "# rotate the image -90 degree to appear right\n",
    "    image_array = rotate(image_array, -90)\n",
    "# from the image from thge array of pixels \n",
    "    new_image = Image.fromarray(image_array)\n",
    "    if display == True:\n",
    "        new_image.show()\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start to visulalize so images from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAADdUlEQVR4nO3VW2/bNhQH8ENSlGRdfFfSxLkUjVOn22C0xTAMe9xQ7BPvKwwFtqzo2m0J6rhp7Tieb7IlRZIlStxD2hVzKMcN9uj/owT9dCgd8gCss84666zzvwWJLmIASFcEJNHzBEO6qiACctsG8nse58InMJFV7sUfb4oA69kB7fzUjsQAMcs77C97CYCM+mPdOnFHgbhmY/sJc9gVywIQVgobVfo1OxYDtFR/xu2462YBRCuUSnmvaIpWBwA8hqLRDLwg4WJA2dzZKqJw5jIxEFy0zx99Z/bskIkBycgbxBn0BpEYSNy/35Tq8f3BZQaAFQUH7/581fIzlgD2L3R/s+l5PsB10/03CBNgs/FoFosB4N5Zy1HrhwWChAAAAE9TcRMAAEDYaV9EB19WVSwEOHCAjC68DvNGnUlx26BiAJJlb7+Od3oGeU0WA0noxymRhNv0Y+bDEc+ZBhECsT32mJIjy4DUDxLZqipCgLmTgatb1QJdAiSMS2ZeXEHiDVv9SqN5qC+rAQCrihhIY/u0nViNhybO/A6SrkucMQ7i82B6TB/tNOcnwzjJANRaTZ2Mx1EGEHYqbX2j0bjqe6I/iqi2db9G547LhEsAYG73+e+08eMPNVVwG2Gz8eRpXfanDsuoIE0nf+j1e0d8qly684QDAAKOCZEIlXVFto6a+9rowvaTDADAfc13H9eK+Vc/t4b+9SqQpOc0o2g9sIqVXas6e3l86WdVABCNz4+xWjnS+UbPSzkHDCAbhmGWrAdW3jT5qHX8ehIDZAwWAKTde/j9V4c525n4MUuBYEL1nKbIsk6lNLx88fL5+WwOmRUAD97HepDula29OEoRkjCRKJUwT+b+lTN49+vpG/vDq8QAAFZKmwdH3+wZlBOVSghBmgRx4vX7b7vng4nrfjhvMioASINgak+SXlFBcl6VCEojfxowr99/2+u67N/+WLZriaKVczLBubKuUDnxnEGQsjD0w5B9GpxLtz0ABkBYLWgylRPfHYc3p/YtAAAChClBGAFLohRgsbdvBT4VQyjl/o1pk/kRb4SYZjV67yxWsDqQ263th1LXmd8VKH379At/87cXw7sCRC3uJt3RycLl1YGrs8pevmwpdwaCjlEuDSeLM3v13yhp+S01cjreXQEAIhOUzBcO2s8BEEKI3z44PzP/AISFe8tsBzdqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x7FB5F1906DD0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pixels_to_image(training_letters.loc[10], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAFBUlEQVR4nO3WWVPbVhQA4HOvVkuyjXe8UBuD7eAAAZrkqdPOZNqZzvTv9qmTvnSmnRJaQhYI4ITEgHcsW/Iia7vqQ7AJxCD3tc2d0YOW8+ncc67sC/BlfBn/r4HQ9Ov0LMEYAACc6bBLLO0RJJ7BAIhYpm4YumZdh1wywOJX6eK8jyUIWQOlcdFqnvXsa8LdACMmN1aKcS/rADaHSqPdrIWqzb5NZp2CP7vx072gh8YOIMeydENvv3v168nAnC0Dio+tbq2lDL1tAgBCjCjy80Ghjd53HWcWgE+s/vAgZp6eHnUQEMzOZeNJIcFQsZ+Hpu0OIMqfW19JaM3d48MOgIPZuVoqm0gGi/rr7oXmDlCe5DePU/bhy6dvFR2BgxCzG0yv/bgYLXxt7s4AcLHFYsapv3j2qjJpvdAaJahspHBRuhhX4XbAt/ZwSTjb++V1+6rxegPzij+Ub29XxlXAt1UAS0vLQXJ6+K42vOq6PWiVTnp0JCax4/7fCtC+pQzbPTzuX/+MrJ4ysDxeDzsOvG0KTCCejCDTkfLhTq9/tXCIadhAszQ1Vm8DxFwx5deYQpyX/zoqda4yA4Q+Hi6AkMkGeVPMOAGFYtoDa1wHxDAMNi1C3AAplxcpkfUBGxK8ZUU1Lq9j3sNRQ92YdHY6gGgxMc9hhuEBc5yabRpjgBa9jDPoaSa5E6A8vqCfAUyIQxAXWevJ6uUdNhRijVZd0cYfw/Q2YprjOQwADnGA8uVycwz+WAHvQtIzatQGkylMBxxwLpcqQgiJy4WoSH2cQHxz3TsolQZX75oKgG0TxwEAjBECNpKI+BkAAC6+mI1j9awycgGIPjItAMAUTWNEC8FEjAMA8K1uLAij1mn1CpheROwROMocdTQDsWKYZ3iflwKg2HChEIDmeUMxXQA+8VVUGJ5vl2UczX2XQpjjMAAbzqzmJfXooDXpwW2AkM76Se3NTkn15AIGAGCEMBvMr2RCjnx8rFrgAvjW133K8+2/a1QiEOYQAHEcNrD8ZGuB6Z8931fhboDmo/k03SxXTF8km1vwgG07bDi4eH8z79XfH35o6i6AJ5XPzeutml6YX8+mwz4y0lFgI/xoecGH67/9WR0RF4AJRIKSjMM4mb6fmOPAtKgI4uMbcV4/29876lrgAlCSSGMu+T2Jznl5BgBs70MiSX7Gqjzd2TsfgRvAhkIs5oIi7ecAAABRksCISLtoHey8rPbBFRCyWYHiWRExlxkJjoPw4Pzt9v5BY3jj4c8BRopnkjya3CC2ZZqaPmy/P9o5ad6MnwJIhY17Kc/klJi9virXaqfV6oU80D97/DqAKD64sLWZ9FFAHMs2LMPQ+hdKR67Wy62OZk/Z5VwHKCH1eH0zHcXIIUavL6tyu1qvdLXRaKTp5rT4axsMWvTFc9/ez/vJqDPUh7IiK3K72qz3P/kRvisDLKVzD+8VohJRyztltdvsjQzD1AzdcqZv0D7NAFEML8XXiluZCNOTy8e/l1W1c3NDdkcGiPKEY8uFR5mgwChHb3ZKZcWyDDJD/GUGnD+WzywtrQUdrX36fH+3MvkjmTEDaeXBk6wkeYlafvb6RUXVLJewmwAXzRYXidGVz0p/HJ4oM0dPAEqSWDJsHB7vfah2PlutMwB2X66NamevSgeNoekSMRXoHztAvTitd/v67LP/FNAbgOyD+uBfvh1g3EbMcJLT02dZOP/B8Q8F4DidavvPRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x7FB5EC8B4610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pixels_to_image(training_letters.loc[200], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAEKElEQVR4nO3W2W/iRhgA8BmPbQw+uAMkhFy7be6FhERq1Updrbqt9o/tc9VuL62qthug5NoSAiEpEO7LDgYfY/pQqdqCwTz0qcr3hOD7fvPNmPEMAI/xGP+LgBY/I4okEU1SBBgZqqSoxngGaQHYvALHuwUnDfCwmal0hvMCBKIZxm6jhaCL491OwQbwsO66zjQ0PN8UKNazvBjxu90LLEXSJEmAkSE3zr+6aI31YNoBpDn3UmhtKeIVGBrqhq5iknc4YGCUqklzAJBwbm3FVxc4O1TFXEuUFWXAb61tOO3uSLEiWgMku3QQi/pGavuhXb+pi7KiaaGFRWNkGHg0Gk82AZilvc93BemycFcpdyVFwwbFeVw+Wu9V8reDOQCS80eW1FrhLFOq9/EIIMYbXg9yei9/cdtU5wAABFCT6n8WbpoKHgHAhHZfPVux9bJfp7M9fQ5Af2jcYLFQbfX6AECKDW4fxldw7er05Loz3oApMCwT9oDeqw4NAADknu692A7j3Pl3mYqkTc7XrAPpPuUklJ6oA8h4IvFoNKCWUolUUZvYCVP+iZBkSWjommIQoY9iH695+ue/v75qyRPPcEoHYKR1//5g8zw5jO/w4m0ycVkxGX4a8E84Y0efPhV6qeQ3122T0a0AmluNHawLnXwicVWdljQL4Hfiz7dc3bfJH3PdqUnTAZpbPTj8wC3mk4lsbUr/MwFu5+DVpk9KJX7Ids2W3wKgHMuxo02/dJNM5Orm6z8bYDcOv9z1SmeJb6+604efCiB7cP9gM6BW04nrqes/C2AW976I+pT785/Sndn15q800r0Z3Q7DxrvTbAWbJFgBpCPy4jgEpfzrk4YxcwFMAUjwa3u765x898fFzcOsBzAFIJjwy+MnAmz8/OudaDWBSQBCPrx/uOPWq+9OM53J94c1QIdeHu0HUfn7k5OSbF0/Adh8G7FnQbKZS6bLommFBcDvxrfD9mbyt1RunvHHAYLy7uyFmIdi+rTYe+9rxJAENPBAn9xV/wYo18pR1NnPJt5cvlcPERcSGIjFe1G1ABDrCQdQJ3NWaBiAAARiKERRDOtZdjEQNzOl2sS8xgAHZ0eD8i9v2wYBCURxASfP+3zLfj+LCKN6knpjAUBIIAQgYsMLNEnRjDPk5gSfb9HDI02RezaKGK8fAwxN1YBj+fl6Bwgs4+A4J0NSlI2h4LBeKxQu8w8WAO63bj1+54cLEiE4GIZjaIgx1rpDuVMqF8rFxsQda+xkQlTks+NPFjHGgEQIIQKoPbnfKVXuK6W2PBioVtc8jBtpzKzaaRpBoGOM9UFT6reL1ftGQzbfmONnI8V517wc53LYDLkvKlhuScPhUBmoypSNOXm40k6Hg3U7aEPuSwoeioPJtmcDBIlIRCEC6Fg3RoY2ea96jMf47+MvAo3ZbSub6u4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x7FB5F1A45D10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pixels_to_image(training_letters.loc[40], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Normalization\n",
    "normalizing mean : to change the intenisty range of pixesl here we will change it from range (0-255)\n",
    "to (0,1) range only to do this we will divide each pixel by 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_letters_scaled = training_letters.values.astype('float32')/255\n",
    "training_letters_labels = training_letters_labels.values.astype('int32')\n",
    "testing_letters_scaled = testing_letters.values.astype('float32')/255\n",
    "testing_letters_labels = testing_letters_labels.values.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images of letters after scaling\n",
      "(13440, 4096)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training images of letters after scaling\")\n",
    "print(training_letters_scaled.shape)\n",
    "training_letters_scaled[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the labels to categories \n",
    "From the labels csv files we can see that labels are categorical values and it is a multi-class classification problem.\n",
    "\n",
    "Our outputs are in the form of:\n",
    "\n",
    "Letters from ’alef’ to ’yeh’ have categories numbers from 10 to 37\n",
    "Here we will encode these categories values using One Hot Encoding with keras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding: : transforms integer to a binary matrix where the array contains only one ‘1’ and the rest elements are ‘0’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# one hot encoding\n",
    "# number of classes = 10 for digits + 28 for charcters \n",
    "number_of_classes = 38\n",
    "training_letters_labels_encoded = to_categorical(training_letters_labels, num_classes=number_of_classes)\n",
    "testing_letters_labels_encoded = to_categorical(testing_letters_labels, num_classes=number_of_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(training_letters_labels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping Input Images to 64x64x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape (nb_samples,rows,columns,channels)\n",
    "\n",
    "where nb_samples corresponds to the total number of images (or samples), and rows, columns, and channels correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "**So we will reshape the input images to a 4D tensor with shape\n",
    "(nb_samples, 64, 64 ,1)** as we use grayscale images of 64x64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13440, 64, 64, 1) (13440, 38) (3360, 64, 64, 1) (3360, 38)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_letters_scaled = training_letters_scaled.reshape([-1, 64, 64, 1])\n",
    "testing_letters_scaled = testing_letters_scaled.reshape([-1, 64, 64, 1])\n",
    "\n",
    "print(training_letters_scaled.shape, training_letters_labels_encoded.shape, testing_letters_scaled.shape, testing_letters_labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make a method which creates the model architecture with the specified optimizer and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Dropout, Dense\n",
    "\n",
    "def create_model(optimizer='adam', kernel_initializer='he_normal', activation='relu'):\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(filters=16, kernel_size=3, padding='same', input_shape=(64, 64, 1), kernel_initializer=kernel_initializer, activation=activation))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=2))\n",
    "  model.add(Dropout(0.2))\n",
    "\n",
    "  model.add(Conv2D(filters=32, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=2))\n",
    "  model.add(Dropout(0.2))\n",
    "\n",
    "  model.add(Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=2))\n",
    "  model.add(Dropout(0.2))\n",
    "\n",
    "  model.add(Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=2))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(GlobalAveragePooling2D())\n",
    "  \n",
    "  #Fully connected final layer\n",
    "  model.add(Dense(38, activation='softmax'))\n",
    "\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=optimizer)\n",
    "  return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
